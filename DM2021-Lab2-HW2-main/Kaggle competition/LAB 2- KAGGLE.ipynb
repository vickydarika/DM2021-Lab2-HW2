{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## LAB II DATA MINING \nThis part is worth 30% of your grade. A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nfrom pandas.io.json import json_normalize","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:30:12.604443Z","iopub.execute_input":"2022-01-09T01:30:12.604794Z","iopub.status.idle":"2022-01-09T01:30:12.610710Z","shell.execute_reply.started":"2022-01-09T01:30:12.604761Z","shell.execute_reply":"2022-01-09T01:30:12.609525Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Preparation\n## 1.1 Load data","metadata":{}},{"cell_type":"code","source":"# read files \ndata_identification = pd.read_csv(\"../input/lab2-data-mining/data_identification.csv\")\nemotion = pd.read_csv(\"../input/lab2-data-mining/emotion.csv\")\nsampleSubmission = pd.read_csv(\"../input/lab2-data-mining/sampleSubmission.csv\")\nraw_data = pd.read_json(\"../input/lab2-data-mining/tweets_DM.json\", lines=True,orient='columns')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:30:12.613158Z","iopub.execute_input":"2022-01-09T01:30:12.613872Z","iopub.status.idle":"2022-01-09T01:30:52.729106Z","shell.execute_reply.started":"2022-01-09T01:30:12.613815Z","shell.execute_reply":"2022-01-09T01:30:52.727979Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# check the identification data\ndata_identification[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:30:52.730739Z","iopub.execute_input":"2022-01-09T01:30:52.731078Z","iopub.status.idle":"2022-01-09T01:30:52.747755Z","shell.execute_reply.started":"2022-01-09T01:30:52.731034Z","shell.execute_reply":"2022-01-09T01:30:52.746546Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# check emotion data\nemotion[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:30:52.750835Z","iopub.execute_input":"2022-01-09T01:30:52.751193Z","iopub.status.idle":"2022-01-09T01:30:52.762411Z","shell.execute_reply.started":"2022-01-09T01:30:52.751144Z","shell.execute_reply":"2022-01-09T01:30:52.761541Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# check the raw data        \nraw_data[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:30:52.764241Z","iopub.execute_input":"2022-01-09T01:30:52.764900Z","iopub.status.idle":"2022-01-09T01:30:52.793850Z","shell.execute_reply.started":"2022-01-09T01:30:52.764854Z","shell.execute_reply":"2022-01-09T01:30:52.792989Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# split the source columns\ndf = pd.json_normalize(data=raw_data['_source'])\n\n# rename the column names of source\ndf=df.rename(index=str,columns={\"tweet.text\":\"text\", \"tweet.tweet_id\":\"tweet_id\",\"tweet.hashtags\":\"hashtags\"})","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:30:52.795254Z","iopub.execute_input":"2022-01-09T01:30:52.796537Z","iopub.status.idle":"2022-01-09T01:31:13.202373Z","shell.execute_reply.started":"2022-01-09T01:30:52.796478Z","shell.execute_reply":"2022-01-09T01:31:13.200979Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# check normalized and renamed raw data\ndf[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:13.203897Z","iopub.execute_input":"2022-01-09T01:31:13.204448Z","iopub.status.idle":"2022-01-09T01:31:13.295756Z","shell.execute_reply.started":"2022-01-09T01:31:13.204395Z","shell.execute_reply":"2022-01-09T01:31:13.294703Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# add identification the dataframe\ndf=pd.merge(df,data_identification, on=\"tweet_id\")","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:13.297446Z","iopub.execute_input":"2022-01-09T01:31:13.297787Z","iopub.status.idle":"2022-01-09T01:31:17.285924Z","shell.execute_reply.started":"2022-01-09T01:31:13.297742Z","shell.execute_reply":"2022-01-09T01:31:17.285133Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Clean text coloumn","metadata":{}},{"cell_type":"code","source":"# clean the text\nimport re\nfrom string import punctuation\n\ndef preprocess_text(text):\n    text = text.lower()  #  lowercase text\n    text = re.sub(f\"[{re.escape(punctuation)}]\", \"\", text)  # remove punctuation\n    text = \" \".join(text.split())  # remove extra spaces, tabs, and new lines\n    return text\n\ndf['text'] = df['text'].map(preprocess_text)\n\n# check the clean text\ndf[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:17.288930Z","iopub.execute_input":"2022-01-09T01:31:17.289467Z","iopub.status.idle":"2022-01-09T01:31:39.133851Z","shell.execute_reply.started":"2022-01-09T01:31:17.289404Z","shell.execute_reply":"2022-01-09T01:31:39.132865Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## 1.3 Train and test data splitting","metadata":{}},{"cell_type":"code","source":"# split into train and test dataset\ntrain_df = df[df['identification']=='train']\ntrain_df = pd.merge(train_df, emotion, on='tweet_id')\n\ntest_df = df[df['identification']=='test']\ntest_df[\"emotion\"]=\"\"","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:39.135198Z","iopub.execute_input":"2022-01-09T01:31:39.135474Z","iopub.status.idle":"2022-01-09T01:31:43.084455Z","shell.execute_reply.started":"2022-01-09T01:31:39.135434Z","shell.execute_reply":"2022-01-09T01:31:43.083488Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# drop the identification and hashtags column\ntrain_df = train_df.drop(['identification'], axis=1)\ntest_df = test_df.drop(['identification'], axis=1)\ntrain_df = train_df.drop(['hashtags'], axis=1)\ntest_df = test_df.drop(['hashtags'], axis=1)\n\n#check the train dataset\ntrain_df[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:43.085813Z","iopub.execute_input":"2022-01-09T01:31:43.086219Z","iopub.status.idle":"2022-01-09T01:31:44.239277Z","shell.execute_reply.started":"2022-01-09T01:31:43.086182Z","shell.execute_reply":"2022-01-09T01:31:44.238430Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#check the test dataset\ntest_df[:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:44.240524Z","iopub.execute_input":"2022-01-09T01:31:44.240834Z","iopub.status.idle":"2022-01-09T01:31:44.253971Z","shell.execute_reply.started":"2022-01-09T01:31:44.240790Z","shell.execute_reply":"2022-01-09T01:31:44.252930Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#group to find distribution\ntrain_df.groupby(['emotion']).count()['text']","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:44.255693Z","iopub.execute_input":"2022-01-09T01:31:44.256014Z","iopub.status.idle":"2022-01-09T01:31:45.003650Z","shell.execute_reply.started":"2022-01-09T01:31:44.255971Z","shell.execute_reply":"2022-01-09T01:31:45.002648Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Histogram emoticon distribution","metadata":{}},{"cell_type":"code","source":"# the histogram of the data\nlabels = train_df['emotion'].unique()\npost_total = len(train_df)\ndf1 = train_df.groupby(['emotion']).count()['text']\ndf1 = df1.apply(lambda x: round(x*100/post_total,3))\n\n#plot\nfig, ax = plt.subplots(figsize=(10, 5))\nplt.bar(df1.index,df1.values)\n\n#arrange\nplt.ylabel('% of instances')\nplt.xlabel('Emotion')\nplt.title('Emotion distribution')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:45.005295Z","iopub.execute_input":"2022-01-09T01:31:45.005547Z","iopub.status.idle":"2022-01-09T01:31:46.185577Z","shell.execute_reply.started":"2022-01-09T01:31:45.005517Z","shell.execute_reply":"2022-01-09T01:31:46.184526Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature Engineering\n## 2.1 TF-IDF Vectorizer & Tweet Tokenizer","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# create a function for the tweet tokenizer from NLTK\ndef tknzr(text):\n    tt = TweetTokenizer()\n    return tt.tokenize(text)\n\n#Use TF-IDF to tokenize document\nvectorizer = TfidfVectorizer(min_df=20, max_df=0.95, ngram_range=(1,1), stop_words='english', tokenizer=tknzr).fit(train_df['text'])\ntfidf_transformed = vectorizer.transform(train_df['text'])","metadata":{"execution":{"iopub.status.busy":"2022-01-09T01:31:46.186800Z","iopub.execute_input":"2022-01-09T01:31:46.187382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Count Vectorizer & Tweet Tokenizer","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import TweetTokenizer\n\n# create a function for the tweet tokenizer from NLTK\ndef tknzr(text):\n    tt = TweetTokenizer()\n    return tt.tokenize(text)\n\ncount_vect = CountVectorizer(max_features=100000, tokenizer=tknzr).fit(train_df['text'])\nbow_transformed = count_vect.transform(train_df['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(bow_transformed, train_df['emotion'], test_size=0.2, random_state=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# take a look at data dimension\nprint('X_train.shape: ', X_train.shape)\nprint('y_train.shape: ', y_train.shape)\nprint('X_test.shape: ', X_test.shape)\nprint('y_test.shape: ', y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\nprint(test_df.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Naive Bayes","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\n# build MultinomialNB model\nNB_model = MultinomialNB()\n\n# train the model\nNB_model = NB_model.fit(X_train, y_train)\n\n# predict the model \ny_train_pred = NB_model.predict(X_train)\ny_test_pred = NB_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check the model accuracy\nfrom sklearn.metrics import accuracy_score\n\nacc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\nacc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n\nprint('training accuracy: {}'.format(round(acc_train, 2)))\nprint('testing accuracy: {}'.format(round(acc_test, 2)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# precision, recall, f1-score,\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_true=y_test, y_pred=y_test_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check by confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \nprint(cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for visualizing confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport itertools\n\ndef plot_confusion_matrix(cm, classes, title='Confusion matrix',\n                          cmap=sns.cubehelix_palette(as_cmap=True)):\n    \"\"\"\n    This function is modified from: \n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n    \"\"\"\n    classes.sort()\n    tick_marks = np.arange(len(classes))    \n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           xticklabels = classes,\n           yticklabels = classes,\n           title = title,\n           xlabel = 'True label',\n           ylabel = 'Predicted label')\n\n    fmt = 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n    ylim_top = len(classes) - 0.5\n    plt.ylim([ylim_top, -.5])\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OUTPUT","metadata":{}},{"cell_type":"code","source":"train_df['emotion'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot your confusion matrix\nmy_tags = ['anticipation', 'sadness', 'fear', 'joy', 'anger', 'trust',\n       'disgust', 'surprise']\nplot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = count_vect.transform(train_df['text'])\ny_train = train_df['emotion']\nX_test = count_vect.transform(test_df['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\nNB_model = NB_model.fit(X_train, y_train)\n\n# predict the model\nprediction = NB_model.predict(X_test)\n\n# save as the output as csv\ntest_df['emotion'] = prediction\noutput = test_df[['tweet_id', 'emotion']].copy()\noutput = output.set_axis([\"id\", \"emotion\"], axis=1)\noutput.to_csv(\"bow-naive-bayes.csv\", index=False)\ntest_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}